# vLLM Blackwell GPU Setup

ğŸš€ **Complete infrastructure for deploying vLLM on cutting-edge Blackwell GPUs with intelligent model routing and smart bypass optimization**

## ğŸ¯ Project Overview

This project provides a complete solution for deploying vLLM (Very Large Language Model) inference server on cutting-edge Blackwell GPUs (RTX 5090, RTX PRO 6000) with Ubuntu 24.04. The setup includes intelligent model routing, smart bypass optimization for real-time conversations, performance optimization, and comprehensive testing.

## âœ¨ Key Features

- **âœ… vLLM Deployment**: Successfully deployed on Blackwell GPUs
- **ğŸ§  Intelligent Model Routing**: Smart model selection based on query type
- **ğŸš€ Smart Bypass Optimization**: Ultra-low latency for real-time conversations
- **ğŸ“Š Performance Optimization**: Optimized for cutting-edge hardware
- **ğŸ”§ Docker Integration**: Complete containerized setup
- **ğŸ“ˆ Monitoring**: Prometheus + Grafana monitoring stack
- **ğŸ§ª Comprehensive Testing**: Full test suite for all use cases
- **ğŸ–¼ï¸ Multimodal Capabilities**: Image and video processing with MiniCPM-V-4
- **ğŸŒ Internal DNS Routing**: Kubernetes cluster access to Docker AI services

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client Apps   â”‚â”€â”€â”€â–¶â”‚  Smart Router   â”‚â”€â”€â”€â–¶â”‚  vLLM Server    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Session Cache  â”‚    â”‚  GPU Memory     â”‚
                       â”‚  (Redis)        â”‚    â”‚  (Pre-loaded)   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ® Use Cases Supported

1. **ğŸš€ Talking Head Avatars & Lip Sync**
2. **ğŸ—£ï¸ Multilingual STT (Indian Languages)**
3. **ğŸµ Multilingual TTS (Indian Languages)**
4. **ğŸ¤– Content Generation & Executing Agents**
5. **ğŸ“Š Multi-Modal Temporal Agentic RAG**
6. **ğŸ¬ Video-to-Text Understanding**

## ğŸ“¦ Models Included

### **âœ… Currently Deployed**
- **MiniCPM-V-4** (7GB) - Multimodal vision-language model with image/video capabilities
- **Whisper Large v3** - Speech-to-text for Indian languages
- **Coqui TTS** - Text-to-speech for Indian languages

### **ğŸ”§ Model Capabilities**
- **Image Processing**: Up to 448x448 pixels with batch processing
- **Video Understanding**: Frame analysis and temporal understanding
- **Multimodal Tasks**: Image+text and video+text analysis
- **Scene Understanding**: Object detection and scene description
- **Gemma-7B-IT** - Google's instruction-tuned model

## ğŸš€ Quick Start

### Prerequisites
- Ubuntu 24.04 LTS
- NVIDIA Blackwell GPUs (RTX 5090, RTX PRO 6000)
- Docker with GPU support
- CUDA 12.9+
- NVIDIA Driver 580.82.07+

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/salilkadam/gpu-setup.git
   cd gpu-setup
   ```

2. **Start the services (Standard)**
   ```bash
   docker-compose up -d
   ```

3. **Start the services (Real-Time Optimized)**
   ```bash
   docker-compose -f docker-compose-realtime.yml up -d
   ```

4. **Test the API**
   ```bash
   curl -X POST http://localhost:8000/v1/completions \
     -H "Content-Type: application/json" \
     -d '{"model": "/app/models/qwen2.5-7b-instruct", "prompt": "Hello, world!", "max_tokens": 50}'
   ```

## ğŸ“Š Performance Results

### **Model Performance**
| Model | Size | Load Time | Inference Speed | Quality |
|-------|------|-----------|-----------------|---------|
| Phi-2 | 5.2GB | 2.1s | Excellent | High |
| Qwen2.5-7B-Instruct | 15GB | 2.3s | Excellent | Superior |

### **Smart Bypass Optimization**
| Metric | Original System | Smart Bypass | Improvement |
|--------|----------------|--------------|-------------|
| **First Request** | 2.3-5.6s | 250-300ms | **90%+ faster** |
| **Ongoing Conversation** | 250-500ms | 200-250ms | **50% faster** |
| **Routing Overhead** | 50-100ms | 1-5ms | **95% reduction** |
| **Bypass Rate** | 0% | 80-95% | **Real-time ready** |

### **Space Optimization**
- **Before**: 155GB (25 models)
- **After**: 20.2GB (2 models)
- **Space Saved**: 135GB (87% reduction)

## ğŸ§ª Testing

### **Run All Tests**
```bash
python3 scripts/test_vllm_compatible_models.py
```

### **Test Smart Bypass Optimization**
```bash
python3 scripts/test_smart_bypass.py
```

### **Test Specific Model**
```bash
python3 scripts/test_single_model.py --model qwen2.5-7b-instruct
```

## ğŸ“ Project Structure

```
gpu-setup/
â”œâ”€â”€ docs/                          # Documentation
â”‚   â””â”€â”€ feature/
â”‚       â””â”€â”€ vllm-blackwell-gpu-setup/
â”‚           â”œâ”€â”€ implementation-plan.md
â”‚           â”œâ”€â”€ implementation-tracker.md
â”‚           â”œâ”€â”€ scratchpad.md
â”‚           â”œâ”€â”€ model-compatibility-report.md
â”‚           â”œâ”€â”€ vllm-compatible-models.md
â”‚           â”œâ”€â”€ performance-comparable-models.md
â”‚           â”œâ”€â”€ latency-analysis-and-optimization.md
â”‚           â””â”€â”€ smart-bypass-optimization.md
â”œâ”€â”€ scripts/                       # Automation scripts
â”‚   â”œâ”€â”€ build-vllm.sh
â”‚   â”œâ”€â”€ download_all_use_case_models.py
â”‚   â”œâ”€â”€ test_all_models_vllm.py
â”‚   â”œâ”€â”€ test_vllm_compatible_models.py
â”‚   â””â”€â”€ test_smart_bypass.py
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ routing/                   # Routing components
â”‚   â”‚   â”œâ”€â”€ smart_bypass_router.py
â”‚   â”‚   â””â”€â”€ realtime_router.py
â”‚   â””â”€â”€ api/                       # API components
â”‚       â””â”€â”€ realtime_routing_api.py
â”œâ”€â”€ docker-compose.yml             # Standard deployment
â”œâ”€â”€ docker-compose-realtime.yml    # Real-time optimized deployment
â”œâ”€â”€ Dockerfile.vllm               # vLLM container
â”œâ”€â”€ prometheus/                   # Monitoring config
â”œâ”€â”€ grafana/                      # Dashboard config
â””â”€â”€ README.md                     # This file
```

## ğŸ”§ Configuration

### **Environment Variables**
```bash
# GPU Configuration
NVIDIA_VISIBLE_DEVICES=all
CUDA_DEVICE_ORDER=PCI_BUS_ID

# vLLM Configuration
VLLM_USE_TRITON_KERNEL=0
VLLM_GPU_MEMORY_UTILIZATION=0.9

# Smart Bypass Configuration
ROUTING_MODE=realtime
REDIS_URL=redis://localhost:6379
```

### **Model Configuration**
Models are stored in `/opt/ai-models/models/` and mounted into containers.

## ğŸ“ˆ Monitoring

Access monitoring dashboards:
- **Grafana**: http://localhost:3000
- **Prometheus**: http://localhost:9090
- **vLLM API**: http://localhost:8000
- **Real-Time Routing API**: http://localhost:8001

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ–¼ï¸ vLLM Multimodal Capabilities

The vLLM endpoint supports advanced multimodal processing with the MiniCPM-V-4 model:

### Image Processing
- **Image Analysis**: Describe and analyze image content
- **Object Detection**: Identify objects in images
- **Scene Understanding**: Understand complex scenes and contexts
- **Format Support**: JPEG, PNG, and other common formats

### Video Understanding
- **Frame Analysis**: Process individual video frames
- **Temporal Analysis**: Understand video sequences over time
- **Action Recognition**: Identify actions and movements
- **Content Summarization**: Summarize video content

### API Usage Examples

#### Direct vLLM Endpoint
```bash
curl -X POST http://192.168.0.21:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/app/models/minicpm-v-4",
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "Describe what you see in this image"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "data:image/jpeg;base64,YOUR_IMAGE_BASE64"
            }
          }
        ]
      }
    ]
  }'
```

#### Through Routing API
```bash
curl -X POST http://192.168.0.21:8001/route \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Analyze this image and describe what you see",
    "use_case": "multimodal"
  }'
```

For detailed information, see [VLLM Multimodal Capabilities Documentation](docs/VLLM-MULTIMODAL-CAPABILITIES.md).

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **vLLM Team** for the excellent inference server
- **NVIDIA** for Blackwell GPU support
- **Hugging Face** for model hosting
- **Qwen Team** for high-performance models

## ğŸ“ Support

For support and questions:
- Create an issue in this repository
- Check the documentation in `docs/`
- Review the implementation tracker

---

**Status**: âœ… **Production Ready with Real-Time Optimization**  
**Last Updated**: December 19, 2024  
**Version**: 2.0.0
