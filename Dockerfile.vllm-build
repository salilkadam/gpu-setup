# vLLM Build Environment for Blackwell GPUs
# Based on Ubuntu 24.04 with CUDA 12.8+ support

FROM nvidia/cuda:12.8-devel-ubuntu24.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    git \
    wget \
    curl \
    build-essential \
    cmake \
    ninja-build \
    pkg-config \
    libssl-dev \
    libffi-dev \
    libbz2-dev \
    libreadline-dev \
    libsqlite3-dev \
    libncursesw5-dev \
    xz-utils \
    tk-dev \
    libxml2-dev \
    libxmlsec1-dev \
    liblzma-dev \
    zlib1g-dev \
    libgdbm-compat-dev \
    libnss3-dev \
    libtirpc-dev \
    && rm -rf /var/lib/apt/lists/*

# Create symbolic links for python
RUN ln -sf /usr/bin/python3.11 /usr/bin/python3 && \
    ln -sf /usr/bin/python3.11 /usr/bin/python

# Install latest NCCL for multi-GPU support
RUN wget https://developer.download.nvidia.com/compute/redist/nccl/v2.26.5/nccl_2.26.5-1+cuda12.8_x86_64.txz && \
    tar -xf nccl_2.26.5-1+cuda12.8_x86_64.txz && \
    cp -r nccl_2.26.5-1+cuda12.8_x86_64/include/* /usr/local/cuda/include/ && \
    cp -r nccl_2.26.5-1+cuda12.8_x86_64/lib/* /usr/local/cuda/lib64/ && \
    rm -rf nccl_2.26.5-1+cuda12.8_x86_64*

# Install PyTorch nightly with CUDA 12.8 support
RUN pip3 install --no-cache-dir \
    torch \
    torchvision \
    torchaudio \
    --index-url https://download.pytorch.org/whl/nightly/cu128

# Install additional build dependencies
RUN pip3 install --no-cache-dir \
    ninja \
    packaging \
    wheel \
    setuptools \
    cmake \
    scikit-build

# Set working directory
WORKDIR /workspace

# Clone vLLM repository
RUN git clone https://github.com/vllm-project/vllm.git && \
    cd vllm && \
    git checkout main

# Build vLLM from source
RUN cd vllm && \
    pip3 install -e . --no-build-isolation

# Install additional vLLM dependencies
RUN pip3 install --no-cache-dir \
    fastapi \
    uvicorn \
    pydantic \
    transformers \
    accelerate \
    sentencepiece \
    protobuf \
    grpcio \
    grpcio-tools

# Create a clean runtime image
FROM nvidia/cuda:12.8-runtime-ubuntu24.04

# Copy Python and vLLM from build stage
COPY --from=0 /usr/local/lib/python3.11 /usr/local/lib/python3.11
COPY --from=0 /usr/local/bin /usr/local/bin
COPY --from=0 /usr/local/lib /usr/local/lib

# Set environment variables
ENV PYTHONPATH=/usr/local/lib/python3.11/dist-packages
ENV PATH=/usr/local/bin:${PATH}
ENV LD_LIBRARY_PATH=/usr/local/lib:${LD_LIBRARY_PATH}

# Create app directory
WORKDIR /app

# Copy vLLM application files
COPY --from=0 /workspace/vllm /app/vllm

# Expose ports
EXPOSE 8000 8001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python3 -c "import vllm; print('vLLM is healthy')" || exit 1

# Default command
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", "--host", "0.0.0.0", "--port", "8000"]
