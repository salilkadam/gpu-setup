# Quick Start Guide: vLLM AI Infrastructure

## üöÄ **Get Started in 5 Minutes**

This guide will help you get up and running with the vLLM AI Infrastructure in just 5 minutes.

## üìã **Prerequisites**

- Docker and Docker Compose installed
- NVIDIA GPU with CUDA support
- At least 32GB GPU memory (RTX 5090 or RTX PRO 6000 recommended)
- Ubuntu 24.04 LTS

## ‚ö° **Quick Setup**

### **Step 1: Clone the Repository**
```bash
git clone https://github.com/salilkadam/gpu-setup.git
cd gpu-setup
```

### **Step 2: Deploy the System**
```bash
# For real-time optimized deployment (recommended)
docker-compose -f docker-compose-realtime.yml up -d

# OR for standard deployment
docker-compose up -d
```

### **Step 3: Wait for Services to Start**
```bash
# Check service status
docker-compose -f docker-compose-realtime.yml ps

# Wait for all services to be healthy (about 2-3 minutes)
```

### **Step 4: Test the API**
```bash
# Test basic functionality
curl -X POST http://localhost:8001/route \
  -H "Content-Type: application/json" \
  -d '{"query": "Hello, world!"}'
```

### **Step 5: Verify System Health**
```bash
# Check system health
curl http://localhost:8001/health

# Check performance stats
curl http://localhost:8001/stats
```

## üéØ **Your First Integration**

### **Python Example**
```python
import requests

# Simple query
response = requests.post('http://localhost:8001/route', json={
    'query': 'Write a Python function to calculate factorial'
})

if response.json()['success']:
    print(response.json()['result'])
else:
    print('Error:', response.json()['error_message'])
```

### **JavaScript Example**
```javascript
// Simple query
fetch('http://localhost:8001/route', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        query: 'Write a Python function to calculate factorial'
    })
})
.then(response => response.json())
.then(data => {
    if (data.success) {
        console.log(data.result);
    } else {
        console.error('Error:', data.error_message);
    }
});
```

### **cURL Example**
```bash
# Basic query
curl -X POST http://localhost:8001/route \
  -H "Content-Type: application/json" \
  -d '{"query": "Write a Python function to calculate factorial"}'

# Query with session continuity
curl -X POST http://localhost:8001/route \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Add error handling to that function",
    "session_id": "my-session-123"
  }'
```

## üéÆ **Supported Use Cases**

### **1. ü§ñ Content Generation & Agents**
```python
# Code generation
response = requests.post('http://localhost:8001/route', json={
    'query': 'Write a Python function to sort a list',
    'modality': 'text'
})

# Business content
response = requests.post('http://localhost:8001/route', json={
    'query': 'Generate a business plan for a tech startup',
    'modality': 'text'
})
```

### **2. üöÄ Avatar & Lip Sync**
```python
# Avatar generation
response = requests.post('http://localhost:8001/route', json={
    'query': 'Generate a talking head avatar with lip sync',
    'modality': 'image'
})
```

### **3. üó£Ô∏è Speech-to-Text**
```python
# STT for Indian languages
response = requests.post('http://localhost:8001/route', json={
    'query': 'Transcribe this audio to text in Hindi',
    'modality': 'audio',
    'context': {'language': 'hindi'}
})
```

### **4. üéµ Text-to-Speech**
```python
# TTS for Indian languages
response = requests.post('http://localhost:8001/route', json={
    'query': 'Convert this text to speech in Tamil',
    'modality': 'audio',
    'context': {'language': 'tamil'}
})
```

### **5. üìä Multimodal RAG**
```python
# Image analysis
response = requests.post('http://localhost:8001/route', json={
    'query': 'Analyze this image and describe what you see',
    'modality': 'image'
})
```

### **6. üé¨ Video Understanding**
```python
# Video analysis
response = requests.post('http://localhost:8001/route', json={
    'query': 'Analyze this video and summarize the content',
    'modality': 'video'
})
```

## üîß **Configuration**

### **Environment Variables**
```bash
# Set in your environment or docker-compose file
export ROUTING_MODE=realtime
export REDIS_URL=redis://localhost:6379
export VLLM_USE_TRITON_KERNEL=0
```

### **Model Configuration**
Models are pre-configured in `src/config/model_registry.yaml`. You can modify:
- Performance scores
- Capabilities
- Supported languages
- Routing preferences

## üìä **Performance Optimization**

### **Smart Bypass Benefits**
- **95% reduction** in routing overhead
- **90%+ faster** first requests
- **80-95% bypass rate** for conversations
- **Real-time ready** with <300ms latency

### **Best Practices**
1. **Use Session IDs**: Maintain conversation continuity
2. **Provide Context**: Include modality hints
3. **Monitor Performance**: Use `/stats` endpoint
4. **Handle Errors**: Implement retry logic

## üö® **Troubleshooting**

### **Common Issues**

#### **Services Not Starting**
```bash
# Check Docker status
docker ps

# Check logs
docker-compose -f docker-compose-realtime.yml logs

# Restart services
docker-compose -f docker-compose-realtime.yml restart
```

#### **GPU Not Detected**
```bash
# Check NVIDIA driver
nvidia-smi

# Check Docker GPU support
docker run --rm --gpus all nvidia/cuda:12.0-base-ubuntu20.04 nvidia-smi
```

#### **API Not Responding**
```bash
# Check service health
curl http://localhost:8001/health

# Check port availability
netstat -tlnp | grep :8001
```

#### **High Latency**
```bash
# Check performance stats
curl http://localhost:8001/stats

# Verify bypass rate is >80%
# Check GPU memory usage
nvidia-smi
```

### **Performance Issues**

#### **Slow Response Times**
- Check if using real-time deployment
- Verify session continuity
- Monitor bypass rate
- Check GPU memory usage

#### **High Memory Usage**
- Monitor Redis memory usage
- Check session count
- Implement session cleanup
- Adjust model memory limits

## üìà **Monitoring**

### **Access Monitoring Dashboards**
- **Grafana**: http://localhost:3000
- **Prometheus**: http://localhost:9090
- **API Stats**: http://localhost:8001/stats

### **Key Metrics to Monitor**
- **Bypass Rate**: Should be >80%
- **Average Response Time**: Should be <300ms
- **Error Rate**: Should be <1%
- **GPU Memory Usage**: Should be <90%

## üîí **Security**

### **Production Checklist**
- [ ] Implement API key authentication
- [ ] Use HTTPS
- [ ] Configure CORS policies
- [ ] Implement rate limiting
- [ ] Set up monitoring
- [ ] Configure secure Redis
- [ ] Implement input validation

## üìö **Next Steps**

### **Learn More**
1. **Read the full documentation**: [App Integration Guide](app-integration-guide.md)
2. **Explore the API**: [API Reference](api-reference.md)
3. **Check examples**: [Integration Examples](app-integration-guide.md#integration-examples)

### **Advanced Features**
1. **Custom Models**: Add your own models to the registry
2. **Custom Backends**: Implement custom inference backends
3. **Performance Tuning**: Optimize for your specific use case
4. **Monitoring**: Set up comprehensive monitoring

### **Production Deployment**
1. **Scale**: Set up load balancing
2. **Security**: Implement authentication and authorization
3. **Monitoring**: Set up alerting and logging
4. **Backup**: Configure backup and recovery

## üÜò **Getting Help**

### **Resources**
- **Documentation**: Check `docs/` folder
- **Issues**: Create GitHub issues
- **Health Check**: Use `/health` endpoint
- **Stats**: Monitor `/stats` endpoint

### **Support**
- Check system health: `curl http://localhost:8001/health`
- Monitor performance: `curl http://localhost:8001/stats`
- Review logs: `docker-compose logs`
- Check GPU status: `nvidia-smi`

## üéâ **Congratulations!**

You're now ready to integrate with the vLLM AI Infrastructure. The system provides:

- ‚úÖ **Ultra-low latency** (200-300ms)
- ‚úÖ **Smart bypass optimization** (95% routing reduction)
- ‚úÖ **Real-time conversations** ready
- ‚úÖ **6 use cases** supported
- ‚úÖ **Multilingual support** (10+ Indian languages)
- ‚úÖ **Production ready** with monitoring

**Happy coding!** üöÄ

---

**Status**: ‚úÖ **Ready for Production**  
**Last Updated**: December 19, 2024  
**Version**: 2.0.0
