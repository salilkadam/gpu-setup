# Multi-stage vLLM Dockerfile for Blackwell GPUs
# Stage 1: Build environment
FROM nvidia/cuda:12.9.0-cudnn-devel-ubuntu24.04 AS vllm-builder

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.12 \
    python3.12-dev \
    python3.12-venv \
    python3.12-full \
    python3-pip \
    git \
    wget \
    curl \
    build-essential \
    cmake \
    ninja-build \
    pkg-config \
    libssl-dev \
    libffi-dev \
    libbz2-dev \
    libreadline-dev \
    libsqlite3-dev \
    libncursesw5-dev \
    xz-utils \
    tk-dev \
    libxml2-dev \
    libxmlsec1-dev \
    liblzma-dev \
    zlib1g-dev \
    libgdbm-compat-dev \
    libnss3-dev \
    libtirpc-dev \
    && rm -rf /var/lib/apt/lists/*

# Create symbolic links for python
RUN ln -sf /usr/bin/python3.12 /usr/bin/python3 && \
    ln -sf /usr/bin/python3.12 /usr/bin/python

# Create and activate virtual environment
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
ENV VIRTUAL_ENV="/opt/venv"

# Install latest NCCL for multi-GPU support (CUDA 12.9 compatible)
RUN wget https://developer.download.nvidia.com/compute/redist/nccl/v2.27.7/nccl_2.27.7-1+cuda12.9_x86_64.txz && \
    tar -xf nccl_2.27.7-1+cuda12.9_x86_64.txz && \
    cp -r nccl_2.27.7-1+cuda12.9_x86_64/include/* /usr/local/cuda/include/ && \
    cp -r nccl_2.27.7-1+cuda12.9_x86_64/lib/* /usr/local/cuda/lib64/ && \
    rm -rf nccl_2.27.7-1+cuda12.9_x86_64*

# Install PyTorch nightly with CUDA 12.9 support
RUN pip install --no-cache-dir \
    torch \
    torchvision \
    torchaudio \
    --index-url https://download.pytorch.org/whl/nightly/cu129

# Install additional build dependencies
RUN pip install --no-cache-dir \
    ninja \
    packaging \
    wheel \
    setuptools \
    setuptools_scm \
    cmake \
    scikit-build

# Set working directory
WORKDIR /workspace

# Clone vLLM repository
RUN git clone https://github.com/vllm-project/vllm.git && \
    cd vllm && \
    git checkout main

# Install additional vLLM dependencies
RUN pip install --no-cache-dir \
    fastapi \
    uvicorn \
    pydantic \
    transformers \
    accelerate \
    sentencepiece \
    protobuf \
    grpcio \
    grpcio-tools

# Build vLLM from source (install in regular mode, not editable)
RUN cd vllm && \
    pip install . --no-build-isolation

# Stage 2: Runtime environment
FROM nvidia/cuda:12.9.0-cudnn-runtime-ubuntu24.04 AS vllm-runtime

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install minimal runtime dependencies
RUN apt-get update && apt-get install -y \
    python3.12 \
    python3.12-venv \
    python3.12-full \
    python3-pip \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create symbolic links for python
RUN ln -sf /usr/bin/python3.12 /usr/bin/python3 && \
    ln -sf /usr/bin/python3.12 /usr/bin/python

# Create app directory
WORKDIR /app

# Copy virtual environment from build stage
COPY --from=vllm-builder /opt/venv /opt/venv

# Activate virtual environment
ENV PATH="/opt/venv/bin:$PATH"
ENV VIRTUAL_ENV="/opt/venv"

# Create model directory
RUN mkdir -p /app/models

# Expose ports
EXPOSE 8000 8001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import vllm; print('vLLM is healthy')" || exit 1

# Default command - vLLM OpenAI-compatible API server
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--host", "0.0.0.0", "--port", "8000"]
