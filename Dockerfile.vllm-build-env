# vLLM Build Environment for Blackwell GPUs
# This is the build environment - use for compilation only

FROM nvidia/cuda:12.8-devel-ubuntu24.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    git \
    wget \
    curl \
    build-essential \
    cmake \
    ninja-build \
    pkg-config \
    libssl-dev \
    libffi-dev \
    libbz2-dev \
    libreadline-dev \
    libsqlite3-dev \
    libncursesw5-dev \
    xz-utils \
    tk-dev \
    libxml2-dev \
    libxmlsec1-dev \
    liblzma-dev \
    zlib1g-dev \
    libgdbm-compat-dev \
    libnss3-dev \
    libtirpc-dev \
    && rm -rf /var/lib/apt/lists/*

# Create symbolic links for python
RUN ln -sf /usr/bin/python3.11 /usr/bin/python3 && \
    ln -sf /usr/bin/python3.11 /usr/bin/python

# Install latest NCCL for multi-GPU support
RUN wget https://developer.download.nvidia.com/compute/redist/nccl/v2.26.5/nccl_2.26.5-1+cuda12.8_x86_64.txz && \
    tar -xf nccl_2.26.5-1+cuda12.8_x86_64.txz && \
    cp -r nccl_2.26.5-1+cuda12.8_x86_64/include/* /usr/local/cuda/include/ && \
    cp -r nccl_2.26.5-1+cuda12.8_x86_64/lib/* /usr/local/cuda/lib64/ && \
    rm -rf nccl_2.26.5-1+cuda12.8_x86_64*

# Install PyTorch nightly with CUDA 12.8 support
RUN pip3 install --no-cache-dir \
    torch \
    torchvision \
    torchaudio \
    --index-url https://download.pytorch.org/whl/nightly/cu128

# Install additional build dependencies
RUN pip3 install --no-cache-dir \
    ninja \
    packaging \
    wheel \
    setuptools \
    cmake \
    scikit-build

# Set working directory
WORKDIR /workspace

# Clone vLLM repository
RUN git clone https://github.com/vllm-project/vllm.git && \
    cd vllm && \
    git checkout main

# Install additional vLLM dependencies
RUN pip3 install --no-cache-dir \
    fastapi \
    uvicorn \
    pydantic \
    transformers \
    accelerate \
    sentencepiece \
    protobuf \
    grpcio \
    grpcio-tools

# Build vLLM from source
RUN cd vllm && \
    pip3 install -e . --no-build-isolation

# Keep container running for interactive use
CMD ["/bin/bash"]
